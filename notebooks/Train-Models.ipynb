{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "# Prétraitement et modélisation\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Modèles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sauvegarde des modèles et des métriques\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Ignorer les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des dossiers pour organiser les artefacts\n",
    "folders = ['data', 'figures', 'models', 'metrics', 'logs']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logger\n",
    "logging.basicConfig(\n",
    "    filename='logs/project_logs.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "logging.info('Initialisation du projet de prédiction du churn client.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données depuis un fichier CSV\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Affichage des premières lignes et enregistrement\n",
    "data.head().to_csv('data/data_preview.csv', index=False)\n",
    "logging.info('Aperçu des données enregistré dans data/data_preview.csv')\n",
    "\n",
    "# Informations sur le DataFrame\n",
    "with open('data/data_info.txt', 'w') as f:\n",
    "    data.info(buf=f)\n",
    "logging.info('Informations sur les données enregistrées dans data/data_info.txt')\n",
    "\n",
    "# Statistiques descriptives et enregistrement\n",
    "data.describe().to_csv('data/data_describe.csv')\n",
    "logging.info('Statistiques descriptives enregistrées dans data/data_describe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage de 'TotalCharges' si nécessaire\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Encodage de la variable cible\n",
    "data['Churn_encoded'] = data['Churn'].map({'Yes': 1, 'No': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables catégoriques\n",
    "categorical_vars = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
    "                    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                    'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Paramètres pour les sous-graphiques\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(categorical_vars) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(categorical_vars):\n",
    "    sns.countplot(x=var, hue='Churn', data=data, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution de {var} par Churn')\n",
    "    axes[idx].set_xlabel(var)\n",
    "    axes[idx].set_ylabel('Nombre')\n",
    "    axes[idx].legend(title='Churn', loc='upper right')\n",
    "\n",
    "# Supprimer les axes vides\n",
    "for ax in axes[len(categorical_vars):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/categorical_barplots.png')\n",
    "plt.close()\n",
    "logging.info('Barplots des variables catégoriques enregistrés dans figures/categorical_barplots.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numériques\n",
    "numerical_vars = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numerical_vars) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(numerical_vars):\n",
    "    sns.boxplot(x='Churn', y=var, data=data, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Boxplot de {var} par Churn')\n",
    "    axes[idx].set_xlabel('Churn')\n",
    "    axes[idx].set_ylabel(var)\n",
    "\n",
    "# Supprimer les axes vides\n",
    "for ax in axes[len(numerical_vars):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/numerical_boxplots.png')\n",
    "plt.close()\n",
    "logging.info('Boxplots des variables numériques enregistrés dans figures/numerical_boxplots.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation\n",
    "corr_matrix = data[numerical_vars + ['Churn_encoded']].corr()\n",
    "\n",
    "# Heatmap de la matrice de corrélation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.savefig('figures/correlation_matrix.png')\n",
    "plt.close()\n",
    "logging.info('Matrice de corrélation enregistrée dans figures/correlation_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des features et de la cible\n",
    "X = data.drop(['Churn', 'customerID', 'Churn_encoded'], axis=1)\n",
    "y = data['Churn_encoded']\n",
    "\n",
    "# Identification des variables binaires et catégoriques\n",
    "binary_vars = [col for col in X.columns if X[col].nunique() == 2]\n",
    "multi_category_vars = [col for col in X.columns if X[col].dtype == 'object' and col not in binary_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du préprocesseur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_vars),\n",
    "        ('bin', OrdinalEncoder(), binary_vars),\n",
    "        ('cat', OneHotEncoder(drop='first'), multi_category_vars)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du préprocesseur sur l'ensemble des données\n",
    "X_normalized = preprocessor.fit_transform(X)\n",
    "\n",
    "# Récupération des noms de colonnes après encodage\n",
    "num_features = numerical_vars\n",
    "bin_features = binary_vars\n",
    "cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(multi_category_vars)\n",
    "all_features = np.concatenate([num_features, bin_features, cat_features])\n",
    "\n",
    "# Conversion en DataFrame\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=all_features)\n",
    "\n",
    "# Enregistrement de la DataFrame normalisée\n",
    "X_normalized_df.to_csv('data/X_normalized.csv', index=False)\n",
    "logging.info('DataFrame normalisée enregistrée dans data/X_normalized.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division initiale en entraînement et temporaire (70% entraînement, 30% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# Division du temporaire en validation et test (50% validation, 50% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "logging.info(f\"Taille de l'entraînement: {X_train.shape}\")\n",
    "logging.info(f\"Taille de la validation: {X_val.shape}\")\n",
    "logging.info(f\"Taille du test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines pour les modèles sans hyperparamètres\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "logging.info('Pipelines scikit-learn créés pour chaque modèle.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour la Régression Logistique\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# Paramètres pour la Forêt Aléatoire\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Paramètres pour le SVM\n",
    "param_grid_svc = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['rbf', 'linear'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(pipeline, param_grid, X_train, y_train, model_name):\n",
    "    logging.info(f\"Début du fine-tuning pour {model_name}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    logging.info(f\"Meilleurs paramètres pour {model_name}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning pour la Régression Logistique\n",
    "best_model_lr = fine_tune_model(pipeline_lr, param_grid_lr, X_train, y_train, 'LogisticRegression')\n",
    "\n",
    "# Fine-tuning pour la Forêt Aléatoire\n",
    "best_model_rf = fine_tune_model(pipeline_rf, param_grid_rf, X_train, y_train, 'RandomForest')\n",
    "\n",
    "# Fine-tuning pour le SVM\n",
    "best_model_svc = fine_tune_model(pipeline_svc, param_grid_svc, X_train, y_train, 'SVC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title='Confusion matrix', cmap=plt.cm.Blues, file_name='confusion_matrix.png'):\n",
    "    \"\"\"\n",
    "    Trace la matrice de confusion personnalisée.\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): Matrice de confusion.\n",
    "        class_names (list): Liste des noms des classes.\n",
    "        title (str): Titre du graphique.\n",
    "        cmap (matplotlib.colors.Colormap): Colormap pour le graphique.\n",
    "        file_name (str): Nom du fichier pour enregistrer le graphique.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Plotting confusion matrix for {title}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        percentage = 100 * cm[i, j] / cm[i, :].sum() if cm[i, :].sum() != 0 else 0\n",
    "        plt.text(j, i, f'{cm[i, j]} ({percentage:.2f}%)',\n",
    "                 horizontalalignment=\"center\", verticalalignment='center',\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, model_name):\n",
    "    logging.info(f'Évaluation du modèle {model_name}')\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_proba)\n",
    "    \n",
    "    # Enregistrement du rapport de classification\n",
    "    with open(f'metrics/classification_report_{model_name}.txt', 'w') as f:\n",
    "        f.write(classification_report(y_val, y_pred))\n",
    "    logging.info(f'Rapport de classification enregistré pour {model_name}')\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    class_names = ['No', 'Yes']\n",
    "    plot_confusion_matrix(cm, class_names, title=f'Matrice de Confusion - {model_name}', file_name=f'figures/confusion_matrix_{model_name}.png')\n",
    "    \n",
    "    # Courbe ROC\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'Courbe ROC - {model_name}')\n",
    "    plt.xlabel('Taux de Faux Positifs')\n",
    "    plt.ylabel('Taux de Vrais Positifs')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'figures/roc_curve_{model_name}.png')\n",
    "    plt.close()\n",
    "    logging.info(f'Courbe ROC enregistrée pour {model_name}')\n",
    "    \n",
    "    # Sauvegarder les métriques\n",
    "    metrics = {\n",
    "        'classification_report': report,\n",
    "        'auc_roc': auc\n",
    "    }\n",
    "    with open(f'metrics/metrics_{model_name}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    logging.info(f'Métriques enregistrées pour {model_name}')\n",
    "    \n",
    "    return report, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaires pour stocker les rapports et les AUC\n",
    "models_reports = {}\n",
    "models_auc = {}\n",
    "\n",
    "# Évaluation du modèle Logistic Regression\n",
    "report_lr, auc_lr = evaluate_model(best_model_lr, X_val, y_val, 'LogisticRegression')\n",
    "models_reports['LogisticRegression'] = report_lr\n",
    "models_auc['LogisticRegression'] = auc_lr\n",
    "\n",
    "# Évaluation du modèle Random Forest\n",
    "report_rf, auc_rf = evaluate_model(best_model_rf, X_val, y_val, 'RandomForest')\n",
    "models_reports['RandomForest'] = report_rf\n",
    "models_auc['RandomForest'] = auc_rf\n",
    "\n",
    "# Évaluation du modèle SVC\n",
    "report_svc, auc_svc = evaluate_model(best_model_svc, X_val, y_val, 'SVC')\n",
    "models_reports['SVC'] = report_svc\n",
    "models_auc['SVC'] = auc_svc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des modèles optimisés\n",
    "joblib.dump(best_model_lr, 'models/model_LogisticRegression.pkl')\n",
    "joblib.dump(best_model_rf, 'models/model_RandomForest.pkl')\n",
    "joblib.dump(best_model_svc, 'models/model_SVC.pkl')\n",
    "\n",
    "logging.info('Modèles optimisés sauvegardés dans le dossier models.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste pour stocker les rapports\n",
    "classification_reports = []\n",
    "\n",
    "for model_name, report in models_reports.items():\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df['model'] = model_name\n",
    "    report_df['metric'] = report_df.index\n",
    "    classification_reports.append(report_df)\n",
    "\n",
    "# Concaténer tous les rapports\n",
    "all_reports_df = pd.concat(classification_reports, axis=0)\n",
    "\n",
    "# Réorganisation des colonnes\n",
    "cols = ['model', 'metric'] + [col for col in all_reports_df.columns if col not in ['model', 'metric']]\n",
    "all_reports_df = all_reports_df[cols]\n",
    "\n",
    "# Enregistrement dans un fichier CSV\n",
    "all_reports_df.to_csv('metrics/all_classification_reports.csv', index=False)\n",
    "logging.info('Tous les rapports de classification enregistrés dans metrics/all_classification_reports.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le meilleur modèle est : LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Poids des métriques\n",
    "weights = {\n",
    "    'precision': 0.25,\n",
    "    'recall': 0.25,\n",
    "    'f1-score': 0.25,\n",
    "    'auc_roc': 0.25\n",
    "}\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "for model_name, report in models_reports.items():\n",
    "    # Récupérer les scores pour la classe positive ('1')\n",
    "    metrics_class_1 = report['1']\n",
    "    auc = models_auc[model_name]\n",
    "    \n",
    "    # Calcul du score final pondéré\n",
    "    final_score = (\n",
    "        weights['precision'] * metrics_class_1['precision'] +\n",
    "        weights['recall'] * metrics_class_1['recall'] +\n",
    "        weights['f1-score'] * metrics_class_1['f1-score'] +\n",
    "        weights['auc_roc'] * auc\n",
    "    )\n",
    "    \n",
    "    benchmark_results[model_name] = {\n",
    "        'precision': metrics_class_1['precision'],\n",
    "        'recall': metrics_class_1['recall'],\n",
    "        'f1-score': metrics_class_1['f1-score'],\n",
    "        'auc_roc': auc,\n",
    "        'final_score': final_score\n",
    "    }\n",
    "\n",
    "# Création de la DataFrame des résultats\n",
    "benchmark_df = pd.DataFrame(benchmark_results).T\n",
    "\n",
    "# Enregistrement de la DataFrame des scores\n",
    "benchmark_df.to_csv('metrics/benchmark_results.csv')\n",
    "logging.info('Résultats du benchmarking enregistrés dans metrics/benchmark_results.csv')\n",
    "\n",
    "# Affichage du meilleur modèle\n",
    "best_model_name = benchmark_df['final_score'].idxmax()\n",
    "print(f\"Le meilleur modèle est : {best_model_name}\")\n",
    "logging.info(f'Le meilleur modèle est : {best_model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles sur les métriques\n",
    "benchmark_df[['precision', 'recall', 'f1-score', 'auc_roc']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparaison des Modèles sur les Différentes Métriques')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Modèle')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_comparison_metrics.png')\n",
    "plt.close()\n",
    "logging.info('Figure de comparaison des métriques enregistrée dans figures/model_comparison_metrics.png')\n",
    "\n",
    "# Figure pour le score final\n",
    "benchmark_df['final_score'].plot(kind='bar', figsize=(8, 6))\n",
    "plt.title('Score Final Pondéré des Modèles')\n",
    "plt.ylabel('Score Final')\n",
    "plt.xlabel('Modèle')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_final_scores.png')\n",
    "plt.close()\n",
    "logging.info('Figure des scores finaux enregistrée dans figures/model_final_scores.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
